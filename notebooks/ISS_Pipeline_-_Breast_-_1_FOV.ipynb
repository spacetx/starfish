{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce In-situ Sequencing results with Starfish\n",
    "\n",
    "This notebook walks through a work flow that reproduces an ISS result for one field of view using the starfish package.\n",
    "\n",
    "## Load tiff stack and visualize one field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from showit import image, tile\n",
    "import pprint\n",
    "%matplotlib inline\n",
    "\n",
    "from starfish.io import Stack\n",
    "\n",
    "s = Stack()\n",
    "s.read('https://dmf0bdeheu4zf.cloudfront.net/ISS/fov_001/experiment.json')\n",
    "# s.squeeze() simply converts the 4D tensor H*C*X*Y into a list of len(H*C) image planes for rendering by 'tile'\n",
    "tile(s.squeeze());  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show input file format that specifies how the tiff stack is organized\n",
    "\n",
    "The stack contains multiple single plane images, one for each color channel, 'ch', (columns in above image) and hybridization round, 'hyb', (rows in above image). This protocol assumes that genes are encoded with a length 4 quatenary barcode that can be read out from the images. Each hybridization encodes a position in the codeword. The maximum signal in each color channel (columns in the above image) corresponds to a letter in the codeword. The channels, in order, correspond to the letters: 'T', 'G', 'C', 'A'. The goal is now to process these image data into spatially organized barcodes, e.g., ACTG, which can then be mapped back to a codebook that specifies what gene this codeword corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8d37d11b3feb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(s.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flat TIFF files are loaded into a 4-d tensor with dimensions corresponding to hybridization round, channel, x, and y. For other volumetric approaches that image the z-plane, this would be a 5-d tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyb, channel, x, y, z\n",
    "s.image.numpy_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show auxiliary images captured during the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'dots' is a general stain for all possible transcripts. This image should correspond to the maximum projcection of all color channels within a single hybridization round. This auxiliary image is useful for registering images from multiple hybridization rounds to this reference image. We'll see an example of this further on in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(s.aux_dict['dots'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a DAPI auxiliary image, which specifically marks nuclei. This is useful cell segmentation later on in the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(s.aux_dict['nuclei'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 4 letter quatenary code (as read out from the 4 hybridization rounds and 4 color channels) represents a gene. This relationship is stored in a codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = pd.read_csv('http://czi.starfish.data.public.s3-website-us-east-1.amazonaws.com/ISS/codebook.csv', dtype={'barcode': object})\n",
    "codebook.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and scale raw data \n",
    "\n",
    "Now apply the white top hat filter to both the spots image and the individual channels. White top had enhances white spots on a black background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.filters import white_top_hat \n",
    "from starfish.viz import tile_lims\n",
    "\n",
    "# filter raw data\n",
    "disk_size = 15  # disk as in circle\n",
    "print(\"filtering tensor\")\n",
    "stack_filt = [white_top_hat(im, disk_size) for im in s.squeeze()]\n",
    "\n",
    "# filter 'dots' auxiliary file\n",
    "print(\"filtering dots\")\n",
    "dots_filt = white_top_hat(s.aux_dict['dots'], disk_size)\n",
    "\n",
    "# convert the unstacked data back into a tensor\n",
    "s.set_stack(s.un_squeeze(stack_filt))\n",
    "s.set_aux('dots', dots_filt)\n",
    "\n",
    "# visualization approach which sets dynamic range to n=2 standard deviations \n",
    "# for an image with total size 10\n",
    "tile_lims(stack_filt, 2, size=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hybridization round, the max projection across color channels should look like the dots stain. \n",
    "Below, this computes the max projection across the color channels of a hybridization round and learns the linear transformation to maps the resulting image onto the dots image. \n",
    "\n",
    "The Fourier shift registration approach can be thought of as maximizing the cross-correlation of two images. \n",
    "\n",
    "In the below table, Error is the minimum mean-squared error, and shift reports changes in x and y dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.pipeline.registration import Registration\n",
    "\n",
    "registration = Registration.fourier_shift(upsampling=1000)\n",
    "registration.register(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spot-detector to create 'encoder' table  for standardized input  to decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pipeline exposes an encoder that translates an image into spots with intensities.  This approach uses a Gaussian spot detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.spots.gaussian import GaussianSpotDetector\n",
    "import warnings\n",
    "\n",
    "    \n",
    "# create 'encoder table' standard (tidy) file format. \n",
    "# takes a stack and exposes a detect method\n",
    "p = GaussianSpotDetector(s)  \n",
    "\n",
    "# parameters to define the allowable gaussian sizes (parameter space)\n",
    "min_sigma = 1\n",
    "max_sigma = 10\n",
    "num_sigma = 30\n",
    "threshold = 0.01\n",
    "\n",
    "# detect triggers some numpy warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # blobs = dots; define the spots in the dots image, but then find them again in the stack. \n",
    "    encoded = p.detect(\n",
    "        min_sigma = min_sigma,\n",
    "        max_sigma = max_sigma, \n",
    "        num_sigma = num_sigma,\n",
    "        threshold = threshold,\n",
    "        blobs = 'dots',\n",
    "        measurement_type='mean',\n",
    "        bit_map_flag=False\n",
    "    )\n",
    "                   \n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualizes a single spot (#100) across all hybridization rounds and channels. It contains the intensity and bit index, which allow it to be mapped onto the correct barcode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[encoded.spot_id == 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder table is the hypothesized standardized file format for the output of a spot detector, and is the first output file format in the pipeline that is not an image or set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spots_df_viz` is produced by the encoder and contains all the information necessary to map the encoded spots back to the original image\n",
    "\n",
    "`x, y` describe the position, while `x_min` through `y_max` describe the bounding box for the spot, which is refined by a radius `r`. This table also stores the intensity and spot_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.spots_df_viz.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each assay type also exposes a decoder. A decoder translates each spot (spot_id) in the Encoder table into a gene (that matches a barcode) and associates this information with the stored position. The goal is to decode and output a quality score that describes the confidence in the decoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are hard and soft decodings -- hard decoding is just looking for the max value in the code book. Soft decoding, by contrast, finds the closest code by distance (in intensity). Because different assays each have their own intensities and error modes, we leave decoders as user-defined functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.pipeline.features.spots.decoder.iss import IssDecoder\n",
    "\n",
    "decoder = IssDecoder()\n",
    "res = decoder.decode(encoded, codebook, letters=['T', 'G', 'C', 'A'])  # letters = channels\n",
    "res.head()\n",
    "\n",
    "# below, 2, 3 are NaN because not defined in codebook. \n",
    "# note 2, 3 have higher quality than 0, 1 (which ARE defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to results from paper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides house keeping genes, VIM and HER2 should be most highly expessed, which is consistent here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.gene.value_counts().sort_index(ascending=False).sort_values(kind='mergesort', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling spots and decoding their gene information, cells must be segmented to assign genes to cells. This paper used a seeded watershed approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.constants import Indices\n",
    "from starfish.watershedsegmenter import WatershedSegmenter\n",
    "\n",
    "dapi_thresh = .16  # binary mask for cell (nuclear) locations\n",
    "stain_thresh = .22  # binary mask for overall cells // binarization of stain\n",
    "size_lim = (10, 10000)\n",
    "disk_size_markers = None\n",
    "disk_size_mask = None\n",
    "min_dist = 57\n",
    "\n",
    "stain = np.mean(s.max_proj(Indices.CH, Indices.Z), axis=0)\n",
    "stain = stain/stain.max()\n",
    "\n",
    "\n",
    "seg = WatershedSegmenter(s.aux_dict['nuclei'], stain)  # uses skimage watershed. \n",
    "cells_labels = seg.segment(dapi_thresh, stain_thresh, size_lim, disk_size_markers, disk_size_mask, min_dist)\n",
    "seg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "This FOV was selected to make sure that we can visualize the tumor/stroma boundary, below this is described by pseudo-coloring `HER2` (tumor) and vimentin (`VIM`, stroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n",
    "# looking at decoded results with spatial information. \n",
    "# \"results\" is the output of the pipeline -- x, y, gene, cell. \n",
    "results = pd.merge(res, p.spots_df_viz, on='spot_id', how='left')\n",
    "\n",
    "rgb = np.zeros(s.image.tile_shape + (3,))\n",
    "rgb[:,:,0] = s.aux_dict['nuclei']\n",
    "rgb[:,:,1] = s.aux_dict['dots']\n",
    "do = rgb2gray(rgb)\n",
    "do = do/(do.max())\n",
    "\n",
    "image(do,size=10)\n",
    "plt.plot(results[results.gene=='HER2'].y, results[results.gene=='HER2'].x, 'or')\n",
    "plt.plot(results[results.gene=='VIM'].y, results[results.gene=='VIM'].x, 'ob')\n",
    "plt.title('Red: HER2, Blue: VIM');"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
